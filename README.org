#+TITLE: WIP - Aristotl
#+AUTHOR: Ben Sima <bensima@gmail.com>
#+DATE: Sunday, April 5, 2015

An API for philosophy data.
  
Sun Mar 15: The current prototype can be viewed at
[[http://aristotl.herokuapp.com][aristotl.herokuapp.com]], but this is an old version, like 30 commits
ago. It's just a Python scraper for [[http://plato.stanford.edu][SEP]] articles.
  
* Rationale
   
  There are many great sources for philosophy papers and readings:
  
  - Extremely high quality articles on the [[http://plato.stanford.edu/][Stanford Encyclopedia of
    Philosophy]]
  - More high quality articles on the [[http://www.iep.utm.edu][University of Tennessee Martin]]
  - [[http://www.jstor.org][JSTOR]] has a ton of papers available, some free, some through
    institutional access.
  - [[http://philpapers.org][Philpapers]] has a huge database of contemporary philosophy papers.
  - Project Gutenberg has a [[http://www.gutenberg.org/wiki/Philosophy_(Bookshelf][philosophy bookshelf]]
  - and many more...
  
  These are free, academic resources for high-quality philosophy. On
  the other hand, popular culture---[[https://news.ycombinator.com/item?id=8709597][especially Silicon Valley tech
  culture]]---does not seem to value academic philosophy. Certain
  figures have become popular, such as Ayn Rand or Alan Turing, but
  the history of philosophy is much richer than pop culture might
  realize.
  
  How can philosophy become as ubiquitous and accessible as computer
  science? My answer to this question is to make philosophy even more
  accessible, in a medium that is familiar to computer scientists and
  software engineers.
  
  Aristotl will accomplish this by exposing a [[https://en.wikipedia.org/wiki/Representational_state_transfer][REST API]] first, and
  later by creating a web app that will provide access to all of the
  data wrapped in an easy-to-use GUI. The API will contain endpoints
  for every philosopher, concept, and discipline in philosophy. Each
  endpoint will provide bibliographies of references collected from
  multiple sources, full-text articles where available, and
  recommendations for similar entries (calculated by the Aristotl
  system). This data will be persisted in a Postgres database
  (transacted by Datomic) and supplied by scraping encyclopedias or
  pulling from the Philpapers API.

* Contributing
   
   I take notes in =.org= files in this repo. I plan features and todos
   in Orgmode and sometimes GitHub issues.
   
   This is a WIP, but if you would like to get involved, you can
   contact me via email or [[https://twitter.com/bensima][twitter]], or just start working on issues. I
   generally try to follow [[https://guides.github.com/introduction/flow/][GitHub Flow]].
   
* Planning
   
   *Current Overarching Goal*: To download all of the SEP articles and
   run a document clustaring algorithm on them.  Because I would like
   to know how all of these articles are related!
   
   This project is to be a system of computation for processing data
   related to philosophy papers, history, etc. I also expect it to be
   state-of-the-art with respect to computer science. After all, I
   should have some fun with this.
   
   My latest idea is to use a microservices architecture and
   distribute these services in a [[https://github.com/svanderburg/disnix][Disnix]] cluster. Each node in the
   cluster should be easy to setup and deploy with a simple script
   (all you have to do is get the [[http://sandervanderburg.blogspot.com/2011/02/disnix-toolset-for-distributed.html][DisnixService]] installed and running
   on a machine, add it to the configuration, and run =disnix-env
   ...=). This would allow easy deployment for people that want to
   support Aristotl but may not be able to program. See for example
   [[https://github.com/kiberpipa/nix-rehash#recontain---herokuhome][nix-rehash]].
   
   I have this down to the following services, each service
   communicating via REST or Websockets, depending on which is more
   appropriate:
   
** =./db/= - Clojure and Datomic Database Service
*** Database service
     
    The database should obviously be Datomic (on top of Cassandra
    perhaps?), and a lightweight Clojure app will run an API service
    to access the data. Routing will be handled by Pedestal (with
    swagger), and really it should only do a few things:
    
    1. Receive data from the Haskell services. The data should be in
       Pandoc native.
    2. Serve data to the world in a variety of formats (Transit/EDN
       first, then later JSON, XML, and HTML).
    3. Do any necessary rate throttling, data checking, etc. There
       shouldn't be much of this to start, however.
     
    For now, it would be best to just dump stuff into the database,
    rebuilding when necessary, but in the future it would be ideal to
    track /all/ of the changes to the database, much in the way that Git
    tracks all changes to source code.  Besides, it would be fun to
    write a diffing algorithm and such. Additionally, the database
    service will act as a /machine learning/ and /analytics/ service
    that can do all of the necessary computations for determining what
    citations and articles are related, etc.
  
**** TODO Record schema
      
     Each record needs to have a UUID at least. The rest depends on the
     kind of record. Thinkers will have different record fields than
     concepts, obviously.
      
**** Interface

     I know I said I should a REST API, but with Datomic's new Pull
     API, it might be most useful to simply expose that
     interface. That way, people could query the database directly for
     what they want.
      
**** TODO [#B] Learn Datomic (schemas, setup, etc)
**** TODO [#C] Define database schema
**** TODO [#C] Get Discovery Service data into the database
** =./api/= - Haskell: Discovery, Parser, and Citation Service
*** Discovery
     
    To "discover" new content, I'll need some kind of a crawler. To
    start, I will simply crawl the SEP's [[http://plato.stanford.edu/contents.html][contents page]] and get the
    information from each of those pages in the form of cleaned
    HTML. (In the future, I can add other sources, such as the
    PhilPapers and JSTOR APIs.) The data will then be sent to the
    Parser (via a simple function call - no need to mess with
    messaging or REST here) and turned into Pandoc Native.
    
    The crawler will not have any public interface to begin with, but
    instead will run like a daemon.
    
    The plan is to:
     
 1. Grab the page via HTTP.
 2. Extract the =#article= section.
 3. Extract specific things inside =#article=, such as:
    1. Images (store in S3)
    2. Metadata (author, copyright, last published, etc)
 4. Identify the core content (e.g. =#main-text= in the SEP) and send
    that to the Parser to be transformed into Pandoc Native
 5. Identify the bibliography (e.g. =#bibliography= in the SEP) and
    send to the Parser to be transformed into the CSL-flavored Pandoc
    Native
 6. After receiving the Pandoc Native from 4 and 5, Discovery will
    combine the content along with the metadata (3.2) and send them to
    the Database.
     
**** TODO [#A] Crawl and scrape the SEP contents page
*** Parser
     
    This must parse the structured text into [[http://johnmacfarlane.net/BayHac2014/doc/pandoc-types/Text-Pandoc-Definition.html][Pandoc Native]], and then
    put it into the database. So basically it's just a wrapper for
    Pandoc.
     
**** TODO [#C] Outline a spec for the Parser Service API
**** TODO [#C] Begin writing the Parser Service API
*** Citations
     
    Parsing citations will not be easy. I can get most of the
    information I need from the HTML markup in the Discovery service,
    but for the parts that I can't get, I'll need some kind of a
    parser, or a /prover/! I could write a Haskell service that takes
    plain-text values and checks them against a citation grammar, such
    as the [[http://istitutocolli.org/repos/citeproc-hs/][Citation Style Language]] (pandoc works with this).
    
    For example,
    
    #+BEGIN_EXAMPLE haskell
    citation = "Paulson, S., 2002, Introduction to Kelsen's Introduction to the Problems of Legal Theory, p. xvii, Oxford: Clarendon Press."
    
    data article = ["a", "an", "the", "to", "of"]
    data pronoun = [[A..Z], [a..z]]
    data name = [[Char], '.']
    
    title :: String -> Char
    title match with title attribues -- I have no idea, it's 4am wtf
    #+END_EXAMPLE

*** Other ideas

    - Since, on the Haskell API side of things, I'll have access to an
      abstract syntax tree of both the article contents /and/ the
      bibliographies, it would be possible to do some kind of static
      analysis.
    - Create a scripting platform with, for example, Lua, to allow
      other people to extract more analyses from the services.
    
* Financial Support
   
  The heavy computation I'm describing will require quite a bit of
  processor power, thus I think financial support is necessary. To
  begin, I think I should bootstrap with just one instance, but then
  scale up manually as necessary. I shall commit to always running one
  instance on my own dime, but I shall go to others if I need more
  instances. To start, these would be manually provisioned, of course,
  but in the future they could be automated with Nix (perhaps even
  with a one-click interface, like Heroku has).
